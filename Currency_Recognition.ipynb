{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "\n",
    "import numpy as np\n",
    "import PIL\n",
    "import PIL.Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1229 files belonging to 6 classes.\n",
      "Using 1168 files for training.\n",
      "Found 1229 files belonging to 6 classes.\n",
      "Using 61 files for validation.\n",
      "['china_coins', 'euro_coins', 'indian_coins', 'peso_coins', 'us_coins', 'yen_coins']\n"
     ]
    }
   ],
   "source": [
    "#https://keras.io/api/data_loading/\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "IMAGE_HEIGHT = 128\n",
    "IMAGE_WIDTH = 256\n",
    "IMAGE_SIZE = (IMAGE_HEIGHT, IMAGE_WIDTH)\n",
    "\n",
    "train_ds = keras.utils.image_dataset_from_directory(\n",
    "    directory='coin_dataset/coins_images/currency_recognition',\n",
    "    labels='inferred',\n",
    "    label_mode='int',\n",
    "    validation_split=0.05,\n",
    "    subset=\"training\",\n",
    "    seed=212,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=IMAGE_SIZE,\n",
    "    crop_to_aspect_ratio=True)\n",
    "\n",
    "val_ds = keras.utils.image_dataset_from_directory(\n",
    "    directory='coin_dataset/coins_images/currency_recognition',\n",
    "    labels='inferred',\n",
    "    label_mode='int',\n",
    "    validation_split=0.05,\n",
    "    subset=\"validation\",\n",
    "    seed=212,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=IMAGE_SIZE,\n",
    "    crop_to_aspect_ratio=True)\n",
    "\n",
    "class_names = train_ds.class_names\n",
    "print(class_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.tensorflow.org/tutorials/load_data/images\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in train_ds.take(1):\n",
    "  for i in range(9):\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "    plt.title(class_names[labels[i]])\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "for image_batch, labels_batch in train_ds:\n",
    "  print(image_batch.shape)\n",
    "  print(labels_batch.shape)\n",
    "  break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.tensorflow.org/tutorials/images/classification#visualize_training_results\n",
    "\n",
    "num_classes = len(class_names)\n",
    "\n",
    "model = Sequential([\n",
    "    layers.Rescaling(1./255, input_shape=(IMAGE_HEIGHT, IMAGE_WIDTH, 3)),\n",
    "    layers.Conv2D(8, (5, 5), padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(16, (3, 3), padding='same', activation='relu'),\n",
    "    layers.Dropout(0.1),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(32, (3, 3), padding='same', activation='relu'),\n",
    "    layers.Dropout(0.1),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.4),\n",
    "    layers.Dense(num_classes)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " rescaling_3 (Rescaling)     (None, 128, 256, 3)       0         \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 128, 256, 8)       608       \n",
      "                                                                 \n",
      " max_pooling2d_9 (MaxPooling  (None, 64, 128, 8)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 64, 128, 16)       1168      \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 64, 128, 16)       0         \n",
      "                                                                 \n",
      " max_pooling2d_10 (MaxPoolin  (None, 32, 64, 16)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 32, 64, 32)        4640      \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 32, 64, 32)        0         \n",
      "                                                                 \n",
      " max_pooling2d_11 (MaxPoolin  (None, 16, 32, 32)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 16384)             0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 128)               2097280   \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 6)                 774       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,104,470\n",
      "Trainable params: 2,104,470\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "  optimizer='nadam',\n",
    "  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "  metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BatchDataset' object has no attribute 'as_numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 12>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ImageDataGenerator\n\u001b[0;32m      3\u001b[0m datagen \u001b[38;5;241m=\u001b[39m ImageDataGenerator(\n\u001b[0;32m      4\u001b[0m         rotation_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m40\u001b[39m,\n\u001b[0;32m      5\u001b[0m         width_shift_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     10\u001b[0m         horizontal_flip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     11\u001b[0m         fill_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnearest\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 12\u001b[0m datagen\u001b[38;5;241m.\u001b[39mfit(\u001b[43mtrain_ds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_numpy\u001b[49m())\n\u001b[0;32m     14\u001b[0m x_batch, y_batch \u001b[38;5;241m=\u001b[39m datagen\u001b[38;5;241m.\u001b[39mflow(train_ds, train_ds, batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'BatchDataset' object has no attribute 'as_numpy'"
     ]
    }
   ],
   "source": [
    "#https://machinelearningmastery.com/image-augmentation-deep-learning-keras/\n",
    "#https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest')\n",
    "datagen.fit(train_ds.as_numpy())\n",
    "\n",
    "x_batch, y_batch = datagen.flow(train_ds, train_ds, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "37/37 [==============================] - 7s 153ms/step - loss: 1.8487 - accuracy: 0.2029 - val_loss: 1.8153 - val_accuracy: 0.0984\n",
      "Epoch 2/15\n",
      "37/37 [==============================] - 6s 152ms/step - loss: 1.7330 - accuracy: 0.2654 - val_loss: 1.7974 - val_accuracy: 0.1639\n",
      "Epoch 3/15\n",
      "37/37 [==============================] - 6s 161ms/step - loss: 1.6472 - accuracy: 0.3373 - val_loss: 1.8073 - val_accuracy: 0.2623\n",
      "Epoch 4/15\n",
      "37/37 [==============================] - 6s 151ms/step - loss: 1.4601 - accuracy: 0.4315 - val_loss: 1.6495 - val_accuracy: 0.3443\n",
      "Epoch 5/15\n",
      "37/37 [==============================] - 6s 157ms/step - loss: 1.2593 - accuracy: 0.5420 - val_loss: 1.5578 - val_accuracy: 0.3443\n",
      "Epoch 6/15\n",
      "37/37 [==============================] - 6s 154ms/step - loss: 0.9134 - accuracy: 0.6695 - val_loss: 1.3859 - val_accuracy: 0.4590\n",
      "Epoch 7/15\n",
      "37/37 [==============================] - 6s 152ms/step - loss: 0.7205 - accuracy: 0.7423 - val_loss: 1.4794 - val_accuracy: 0.5246\n",
      "Epoch 8/15\n",
      "37/37 [==============================] - 6s 158ms/step - loss: 0.5120 - accuracy: 0.8339 - val_loss: 1.2577 - val_accuracy: 0.5902\n",
      "Epoch 9/15\n",
      "37/37 [==============================] - 6s 149ms/step - loss: 0.3750 - accuracy: 0.8836 - val_loss: 1.1796 - val_accuracy: 0.7213\n",
      "Epoch 10/15\n",
      "37/37 [==============================] - 6s 156ms/step - loss: 0.2548 - accuracy: 0.9272 - val_loss: 1.3224 - val_accuracy: 0.6721\n",
      "Epoch 11/15\n",
      "37/37 [==============================] - 7s 169ms/step - loss: 0.1894 - accuracy: 0.9452 - val_loss: 1.3932 - val_accuracy: 0.6557\n",
      "Epoch 12/15\n",
      "37/37 [==============================] - 6s 155ms/step - loss: 0.1486 - accuracy: 0.9546 - val_loss: 1.2736 - val_accuracy: 0.6721\n",
      "Epoch 13/15\n",
      "37/37 [==============================] - 6s 157ms/step - loss: 0.1395 - accuracy: 0.9598 - val_loss: 1.4118 - val_accuracy: 0.6393\n",
      "Epoch 14/15\n",
      "37/37 [==============================] - 6s 151ms/step - loss: 0.1376 - accuracy: 0.9563 - val_loss: 1.2185 - val_accuracy: 0.7377\n",
      "Epoch 15/15\n",
      "37/37 [==============================] - 6s 157ms/step - loss: 0.3942 - accuracy: 0.8913 - val_loss: 1.2999 - val_accuracy: 0.6557\n"
     ]
    }
   ],
   "source": [
    "epochs=15\n",
    "history = model.fit(\n",
    "  train_ds,\n",
    "  validation_data=val_ds,\n",
    "  epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Sequential' object has no attribute 'predict_classes'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_classes\u001b[49m(val_ds)\n\u001b[0;32m      2\u001b[0m con_mat \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mmath\u001b[38;5;241m.\u001b[39mconfusion_matrix(labels\u001b[38;5;241m=\u001b[39mlabels, predictions\u001b[38;5;241m=\u001b[39my_pred)\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Sequential' object has no attribute 'predict_classes'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "9eeadce64ad6362279a6e56cb0d4c300d68f82e710edde82368f3afc68363566"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
